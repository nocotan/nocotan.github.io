<!DOCTYPE html>
<html>    
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width initial-scale=1">

  <title>GANsの目的関数とJS divergence</title>
  <meta name="description" content="Generative Adversarial Networksの目的関数がJensen-Shannon divergenceを用いて表現できることの証明．">
  <meta name="google-site-verification" content="jhcQ391AbIJxLfLGHuUvmNEmr1luyROr-4GbAYF4mNE" />
  <meta name="twitter:card" content="summary">
  <meta name="twitter:site" content="@nocotan">
  <meta name="twitter:creator" content="@nocotan">
  
    <meta name="twitter:title" content="GANsの目的関数とJS divergence">
  
  
    <meta name="twitter:url" content="http://localhost:4000/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92/2018/05/30/gans_jsd-copy.html">
  
  
    <meta name="twitter:description" content="雑記
">
  
  <meta name="twitter:description" content="Generative Adversarial Networksの目的関数がJensen-Shannon divergenceを用いて表現できることの証明．

証明
Jensen-Shannon Divergence
Jensen-Shannon Divergenceは以下の式で表現される．



ここで$D_{KL}$はKL divergenceであり，



となる．$p_{A} = \f...">
  <meta name="twitter:image" content="https://raw.githubusercontent.com/nocotan/nocotan.github.io/master/images/icon.jpg">
  <link rel="shortcut icon" type="image/x-icon" href="/images/favicon.ico">
  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="http://localhost:4000/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92/2018/05/30/gans_jsd-copy.html">
  <link rel="alternate" type="application/atom+xml" title="nocohub" href="http://localhost:4000/feed.xml" />
  <script src="/scripts/jquery-1.11.2.min.js"></script>
  <script src="/scripts/pithy.js"></script>
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-74313651-2', 'auto');
  ga('send', 'pageview');
  </script>
</head>


  <body>
    <header class="header">
	<div class="header-container">
		<div class="nav">
			
				<li>
					<a href="/index">home</a>
				</li>			
			
			
				<li>
					<a href="/archive">archive</a>
				</li>			
			
			
				<li>
					<a href="/category">category</a>
				</li>			
			
			
				<li>
					<a href="/about">about</a>
				</li>			
			
		</div>
		<div class="description">  </div>		
		<ul class="social-links">
			<li>
				<a href="https://github.com/nocotan" title="Github">
					<img width="19px" height="19px" src="/images/github.png"/>
				</a>
			</li>
			<li>
				<a href="/feed.xml" title="RSS">
					<img width="19px" height="19px" src="/images/rss.png"/>
				</a>
			</li>
			<li>
				<a href="https://twitter.com/machinery81" title="Twitter">
					<img width="19px" height="19px" src="/images/twitter.png"/>
				</a>
			</li>
		</ul>
	</div>
</header>

    <br>
    <div class="page-content">
      <div class="wrapper">
        <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
      tex2jax: {inlineMath: [['$','$']]}
    });
</script>
<div class="post">
  <br>
  <header class="post-header">
    <h1 class="post-title">GANsの目的関数とJS divergence</h1>
    <p class="post-meta">May 30, 2018</p>
  </header>

  <article class="post-content">
    <p>Generative Adversarial Networksの目的関数がJensen-Shannon divergenceを用いて表現できることの証明．</p>

<h2 id="証明">証明</h2>
<h3 id="jensen-shannon-divergence">Jensen-Shannon Divergence</h3>
<p>Jensen-Shannon Divergenceは以下の式で表現される．</p>

<script type="math/tex; mode=display">\begin{equation}
\tag{1}
D_{JS}(p||q) = \frac{1}{2} D_{KL}(p||\frac{p+q}{2}) + \frac{1}{2} D_{KL}(q||\frac{p+q}{2})
\end{equation}</script>

<p>ここで$D_{KL}$はKL divergenceであり，</p>

<script type="math/tex; mode=display">\begin{equation}
\tag{2}
D_{KL}(p||q) = \int_x p(x) \log{\frac{p(x)}{q(x)}} dx
\end{equation}</script>

<p>となる．$p_{A} = \frac{p+q}{2}$とおくと，式(1)と(2)から，</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{eqnarray}
\tag{3}
D_{JS}(p||q) &=& \frac{1}{2} D_{KL}(p||p_{A}) + \frac{1}{2} D_{KL}(q||p_{A}) \\
&=& \frac{1}{2} \int_x p(x) \log{\frac{p(x)}{p_A(x)}} dx + \frac{1}{2} \int_x q(x) \log{\frac{q(x)}{p_A(x)}} dx
\end{eqnarray} %]]></script>

<h3 id="generative-adversarial-networks">Generative Adversarial Networks</h3>
<p>GANsの目的関数は以下の形で表現される．</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{eqnarray}
\tag{4}
\min_{G}\max_{D} L(D, G) &=& \mathbb{E}_{x\sim{p_r(x)}}[\log{D(x)}] + \mathbb{E}_{z\sim{p_z(z)}}[\log{(1-D(G(z)))}] \\
&=& \mathbb{E}_{x\sim{p_r(x)}}[\log{D(x)}] + \mathbb{E}_{x\sim{p_g(x)}}[\log{(1-D(x))}]
\end{eqnarray} %]]></script>

<p>この式について，Discriminatorは最小化を目指し，Generatorは最大化を目指す．
式(4)を変形すると，</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{eqnarray}
\tag{5}
L(G, D) &=& \int_x p_r(x)\log{(D(x))} dx + \int_x p_g(x)\log{(1-D(x))} dx \\
&=& \int_x p_r(x)\log{(D(x))} + p_g(x)\log{(1-D(x))} dx
\end{eqnarray} %]]></script>

<p>ここで，Discriminator $D(x)$の最適解を考えてみる．
$\tilde{x} = D(x)$，$a=p_r(x)$, $b=p_g(x)$と置き，インテグラルの中を$\tilde{x}$についての関数$f(\tilde{x})$とすると，</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{eqnarray}
\tag{6}
f(\tilde{x}) &=& a\log{(\tilde{x})} + b\log{(1-\tilde{x})} \\
\frac{df(\tilde{x})}{d\tilde{x}} &=& \frac{a}{\tilde{x}} - \frac{b}{1-\tilde{x}} \\
&=& \frac{a-(a+b)\tilde{x}}{\tilde{x}(1-\tilde{x})}
\end{eqnarray} %]]></script>

<p>$\frac{df(\tilde{x})}{d\tilde{x}} = 0$とおくと，</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{eqnarray}
\tag{7}
\frac{a-(a+b)\tilde{x}}{\tilde{x}(1-\tilde{x})} &=& 0 \\
a - (a+b)\tilde{x} &=& 0 \\
\tilde{x} &=& \frac{a}{a+b} \\
&=& \frac{p_r(x)}{p_r(x) + p_g(x)}
\end{eqnarray} %]]></script>

<p>式(5)と式(7)から，$D(x)$が最適解$\hat{D}(x)$であるときの$L(G, \hat{D})$は，</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{eqnarray}
\tag{8}
L(G, \hat{D}(x)) &=& \int_x p_r(x) \log{(\frac{p_r(x)}{p_r(x) + p_g(x)})} + p_g(x)\log{(1 - \frac{p_r(x)}{p_r(x) + p_g(x)})} dx \\
&=& \int_x p_r(x) \log{(\frac{p_r(x)}{2p_A(x)})} + p_g(x)\log{(1 - \frac{p_r(x)}{2p_A(x)})} dx \\
&=& \int_x p_r(x) \log{\frac{p_r(x)}{p_A(x)}} + p_g(x)\log{\frac{p_r(x)}{p_A(x)}} dx - 2\log{2} \\
&=& 2(\frac{1}{2}\int_x p_r(x) \log{\frac{p_r(x)}{p_A(x)}} + p_g(x)\log{\frac{p_r(x)}{p_A(x)}} dx) - 2\log{2} \\
&=& 2D_{JS}(p_r||p_g) - 2\log{2}
\end{eqnarray} %]]></script>

<p>以上から，GANsはDiscriminatorが最適である場合，本質的にはJS divergenceによってデータ分布$p_r$とGeneratorの生成するサンプルの分布$p_g$との距離に基づいて最適化を行なっていることがわかる．</p>

  </article>
</div>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

        <br><br>
        <a href="http://b.hatena.ne.jp/entry/" class="hatena-bookmark-button" data-hatena-bookmark-layout="vertical-normal" data-hatena-bookmark-lang="ja" title="このエントリーをはてなブックマークに追加"><img src="https://b.st-hatena.com/images/entry-button/button-only@2x.png" alt="このエントリーをはてなブックマークに追加" width="20" height="20" style="border: none;" /></a><script type="text/javascript" src="https://b.st-hatena.com/js/bookmark_button.js" charset="utf-8" async="async"></script>
        <a href="https://twitter.com/share" class="twitter-share-button" data-via="machinery81">Tweet</a><script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
      </div>
    </div>
    
    <footer class="footer">
  <div id="gotop">^</div>
  <br>
	@2015 Pithy Theme by Pawpaw.
</footer>

    
  </body>

</html>
