---
layout: post
title:  "node2vec"
date:   2017-07-01 01:00:00
categories: 機械学習
---

KDD2016に採択された論文, [node2vec: Scalable Feature Learning for Networks](https://cs.stanford.edu/people/jure/pubs/node2vec-kdd16.pdf)について.  
目標は, オブジェクト集合の特徴を学習すること.  
提案手法はあるタスク特有のものではなく, 任意のタスクに適用可能なものである.  
また, 有向, 無向, 重み付き, 重み無しのグラフに適用可能.

## グラフ上での特徴学習
グラフG = (V, E)が与えられる. ここで, Vはノード集合, Eはエッジ集合とする.
また, マッピング関数fを以下のように定義する:  
<br>
<strong>　　f: V → R<sup>d</sup></strong> 　　(1)  
<br>

直感的に, 何らかノードの類似性を保持したまま, ノードからd次元の特徴へのマッピングを見つけたいことがわかる.
単純なアイディアとして, 近くのノードが近くに存在するようにノードの埋め込みを学習する.
ノードu ∈ Vが与えられた時, 近傍サンプリングSから導かれるノードuの近傍をN<sub>S</sub>(u)と定義する. 
この近傍ノードN<sub>S</sub>(u)を予測することによって最適な重みを見つけたい(尤度最大化問題).
よって, 以下の目的関数を最適化することを目指す.
これはfによって与えられた特徴表現を条件とするノードuのネットワーク近傍N<sub>S</sub>を観測する対数確率を最大化する問題に帰結する.

<br>
　　<strong>max Σ log Pr(N<sub>S</sub>(u) | f(u))</strong>　　(2)  
<br>

ここで, 最適化問題を扱いやすくするために, 二つの仮定を導入する.

* 条件付き独立である
* 特徴空間において対称である

これらの基本的な仮定に基づいて, Prは以下のように定義される.

<br>
　　<strong>Pr(N<sub>S</sub>(u) | f(u)) = ∏ Pr(n<sub>i</sub> | f(u))</strong>　　(3)  
　　<strong>Pr(n<sub>i</sub> | f(u)) = exp(f(n<sub>i</sub>)・f(u)) / Σ exp(f(v)・f(u))</strong>　　(4)  
<br>

以上より, 式2は以下のように単純化できる:

<br>
　　<strong>max Σ [ -logZ<sub>u</sub> + Σ f(n<sub>i</sub>)・f(u)]</strong>　　(5)  
<br>

ノード分割関数Z<sub>u</sub> = Σ(f(u)・f(v))は大規模ネットワーク上で計算でき, ネガティブサンプリングを用いてこれを計算できる.
式5により, 確率的勾配降下法(SGD: Stochastic Gradient Descent)を用いてf(u)を最適化する.

## 近傍ノードの決定
与えられたノードの近傍ノードを決定するために, 幅優先探索(BFS)と深さ優先探索(DFS)という2つの古典的な探索手法がある.
近傍3ノードを探索する際のDFSとBFSの動作を図1に示す.
特に, グラフ上でのノード予測タスクは二種類の類似度に基づく(構造的等価性と共起表現的等価性).
共起表現的等価性の下では, 相互に接続されており, 同様のクラスタまたはコミュニティに属するノードは密に埋め込まれるべきである.
対照的に, 構造上の等価性仮説の下ではグラフにおいて類似の構造的役割を有するノードが密に埋め込まれるべきである.  
重要なことに, 共起表現とは異なり, 構造上の同等性は接続性を重要視していない.
これは, ノードはグラフ上では遠く離れていても, 同じ構造的役割を持っていることがあるためである.
実世界において, これらの等価概念は排他的ではない.
グラフは, いくつかのノードが共起表現的等価性を示し, 他のノードが構造的等価性を示す.
BFSとDFSのどちらを採用するかは構造的等価性と共起表現的等価性のどちらを優先するかで決まる.
