---
layout: post
title:  "Leaf-Smoothed Hierarchical Softmax for Ordinal Prediction"
date:   2018-01-09 20:00:00
categories: 機械学習
---

元論文: [http://www.cs.utexas.edu/~tansey/aaai18-lshs.pdf](http://www.cs.utexas.edu/~tansey/aaai18-lshs.pdf)

## 概要
階層的ソフトマックス(hSm: Hierarchical Softmax)の特殊系に関する研究．
hSmの一般的な代替モデルとしてLeaf-smoothed Hierarchical Softmax(LSHS)を提案．
LSHSは，最初にkd木に類似した方法で出力確率の階層的分解を用いて順序ラベル空間の構造を利用する．
これによってモデル全体のサンプル効率は向上するが，一部空間でのエラーが上昇してしまう，
これを克服するため，対象値の周辺領域を平滑化する正規化手法を導入する．

## 貢献
* 既存のCPEモデルの詳細な分析
* 大規模なドメインに対するスケーラブルなグラフベースの正規化手法を提案
* 深層学習モデルを用いた離散確率分布を推定するノンパラメトリックな手法であるLeaf-Smoothed Hierarchical Softmaxを提案．

## Hierarchical softmax variant
提案手法では，kd木から着想を得た領域分割の手法を用いてhSmを効率化する．

<div align="center">
<img src="/images/20180109/lshs_fig1.png" width="500px">
</div>

### Dyadic decomposition
多項分布から直接確率を出力するのではなく，ルートノードが順序空間の中心となるような平衡二分木を生成する．

* ルートノードから順序空間を再帰的に二分割していく
* サイズnの空間から得られる木はn-1個のノードをもつ
* Deep Learningモデルは独立した二値分類の結果列を出力する

$$
p(y > b_i | x) = \frac{1}{1 + exp(- \epsilon_i)}
$$

ここで，$$b_i$$はノードの中心となる．

このような構造を利用することで，計算量の効率化ができる．
これを用いることで，与えられた学習例に対する確率の推論において，ルートから$$O(\log_{2} n)$$のパスのみの計算ですむ．

### Multiple dimensions
提案手法では，hierarchical softmaxの手法を，平衡kd木に類似した方法を用いて多次元分布へ拡張する．
これは木の各レベルで次元を交互に変えながら，木の分割を幅優先的に列挙することで実現する．

## Leaf smoothing
上記の木構造を用いる場合，パーティションを跨いだ近傍ラベルとの間に局所的な偏りが生じてしまう．

Figure 1を例に説明する．
例えば，$$y_i = 4$$の場合，ノードAとCもリーフ5に向いているため，結果として$$p(y_i = 5 | x_i)$$の確率も増加する．
一方で$$p(y_i = 3)$$に関しては，ノードAがリーフ3とは逆をむくため，確率が下がることがわかる．
この更新時の不均衡は，基礎となる条件付き分布の推定におけるギザギザした誤差分布を引き起こす．

これに対処するため，特定のパスだけでなく，目標の近傍を考慮した平滑化を導入する．

<div align="center">
<img src="/images/20180109/lshs_fig2.png" width="800px">
</div>

### Trend filtering logits
観測点同士の依存関係をグラフ構造として表現する問題は，様々な分野で広く研究されているテーマである．
こうした問題に対するものの中に，ノイズを含む観測点からなる既知のグラフ構造から，隣接点同士の潜在的な関係を利用するものがある．
これによって，全ノード間についての関係を調べる必要がなくなるため，効率的である．

Trend filtering (TF)は，以下の凸最適化問題を解くために提案されたデノイジング技術である．

$$
minimize_{\beta \in \mathcal{R}^N} \ \ \ell (y, \beta) + \lambda || \Delta^{(k)} \beta ||_{1}
$$

$$y$$はノイズの多い観測ベクトルである．

提案手法では，ラベル空間からd次元格子グラフを導出することでTFを適用する．
上式を変形して得られる損失関数は以下のようになる．

$$
L_i = - \log [p(y = y_i|x_i)] + \lambda || \Delta^{(k)} vec (\log [p(y|x_i)]) ||_1
$$

### Local smoothing via neighborhood sampling
TFによって全点間の平滑化を行うのは時間的計算量がかかってしまうため，目的値の近傍のみを平滑化する．
具体的には$$y_i$$が与えられた時，出力空間内で$$y_i$$の半径$$r$$のノードについて平滑化を行う．

$$
L_i = - log [p(u = y_i | x_i)] + \lambda || \tilde{\Delta{}}^{(k)} \ell (y_i, x_i) ||_1
$$

### 実験結果

<div align="center">
<img src="/images/20180109/lshs_fig3.png" width="800px">
</div>

## 参考文献
Tansey, Wesley, Karl Pichotta, and James G. Scott. "Leaf-Smoothed Hierarchical Softmax for Ordinal Prediction." (2018).
